<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- DELETE THIS SCRIPT if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- Global site tag (gtag.js) - Google Analytics -->


  <title>Soham Gadgil</title>
  
  <meta name="author" content="Soham Gadgil">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css" rel="stylesheet">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"></script>
  <script>
  $(document).ready(function(){
      $('[data-toggle="tooltip"]').tooltip({
          placement : 'bottom'
      });
  });
  </script>
  <style>
    /* Customizing the tooltip */
    .tooltip-inner {
        background-color: black; /*Change background color*/
        color: white; /* Change text color */
        border-radius: 10px; /* Rounded corners */
        font-size: 12x; /* Increase font size */
        padding: 10px; /* Add padding */
        box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); /* Add a shadow */
    }

    /* Tooltip arrow customization */
    .tooltip.bs-tooltip-top .tooltip-arrow {
        border-top-color: #4CAF50; /* Match arrow color to the tooltip */
    }

    /* Animation for tooltip */
    .fade {
        transition: opacity 0.5s ease-in-out; /* Smooth fade-in/out effect */
    }
</style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Soham Gadgil</name>
              </p>
              <p>
              Hi! I am a third year CSE PhD 
              student at University of Washington in the <a href="https://aims.cs.washington.edu/home" target="_blank">AIMS Lab</a>, co-advised by <a href="https://www.linkedin.com/in/su-in-lee-b9855073/" target="_blank">Dr. Su-In Lee</a> and <a href="https://homes.cs.washington.edu/~shapiro/" target="_blank">Dr. Linda Shapiro.</a> Prior to starting my Ph.D, I spent a year as a data engineer at Microsoft in the Windows Experience team.
              </p>
              <p class="content">
              I completed my Masters from Stanford in Computer Science with a depth in AI, where I was a research asistant in the Computational Neuroimage Science Lab (<a href="https://cnslab.stanford.edu/">CNS<sup>LAB</sup></a>), advised by <a href="https://cnslab.stanford.edu/personal/kilian-pohl">Dr. Kilian Pohl</a>. I was also part of the AI for Healthcare (<a href="https://stanfordmlgroup.github.io/programs/aihc-bootcamp/" target="_blank">AIHC</a>) bootcamp in the Stanford ML Group, advised by <a href="https://pranavrajpurkar.com/" target="_blank">Dr. Pranav Rajpurkar</a> and <a href="https://www.andrewng.org/" target="_blank">Dr. Andrew Ng</a>. I finished my Bachelor's at Georgia Institute of Technology, with a major in Computer Engineering and a minor in Computer Science.
              </p>
              <p class="content">I am also a part-time instructor at <a href="https://persolv.ai/" target="_blank">Persolv</a>, teaching AI fundamentals to high school students. In my spare time, I like playing tennis, hiking, exploring different cuisines, and watching movies (especially legal thrillers).
              <p style="text-align:center">
                <a href="mailto:sgadgil@uw.edu"><span class="bi bi-envelope-at" data-toggle="tooltip" data-original-title="Email" style="font-size: 2rem;"></span></a> &nbsp&nbsp
                <a href="data/Resume_Soham_Gadgil_PhD_Short.pdf" target="_blank"><span class="bi bi-file-pdf" data-toggle="tooltip" data-original-title="CV" style="font-size: 2rem;"></span></a></a> &nbsp&nbsp
                <a href="https://scholar.google.com/citations?user=Tg2VT2UAAAAJ&hl=en&oi=ao" target="_blank"><i class="ai ai-google-scholar ai-2x" data-toggle="tooltip" data-original-title="Google Scholar" style="font-size: 2rem;"></i></a> &nbsp&nbsp
                <a href="https://github.com/sgadgil6" target="_blank"><span class="bi bi-github" data-toggle="tooltip" data-original-title="GitHub" style="font-size: 2rem;"></span></a> &nbsp&nbsp
                <a href="https://twitter.com/soham_gadgil" target="_blank"><span class="bi bi-twitter-x" data-toggle="tooltip" data-original-title="Twitter" style="font-size: 2rem;"></span></a> &nbsp&nbsp
                <a href="https://www.linkedin.com/in/soham-gadgil-2729a9105/" target="_blank"><span class="bi bi-linkedin" data-toggle="tooltip" data-original-title="LinkedIn" style="font-size: 2rem;"></span></a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_headshot.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_headshot.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <hr>
              <p>
                The research problems I want to work on lie at the interesection of Artificial Intelligence and Healthcare. Some of my current research interests include: 
              </p>
              <p class="content">
              <ul>
                <li><b><i>Clinical AI:</i></b> Using multi-modal data (images, text, etc.) with deep learning to perform diagnoses and treatment of various ailments. </li> 
                <li><b><i>Explainability:</i></b> Developing AI techniques to increase model interpretability in settings where features are not trivial to obtain (like Emergency Medicine), auditing AI models to make them more trustworthy by exploring causal relationships between inputs and predictions. </li> 

              </ul>
              </p>

            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td>
              <heading>Achievements</heading>
              <hr>
              <ul>
                <li>Won the best paper runner-up award at the CVPR 2024 <a href="https://dca-in-mi.github.io/" target="_blank">DCAMI Workshop</a>.</li>
                <li>Accepted into the ORS (<a href="https://ors.ece.gatech.edu/" target="_blank">Opportunity Research Scholars</a>) program for undergraduate research at Georgia Tech.</li>
                <li>Awarded <a href="https://registrar.gatech.edu/info/faculty-honors-letters" target="_blank"> Faculty Honors </a> by Georgia Tech for all semesters in my undergraduate degree. </li>
                <li>Recipient of the Electrical and Computer Engineering (ECE) Senior award for highest academic average.</a></li>
                <li>Won the best <a href="https://web.stanford.edu/class/aa228/reports/2019/final8.pdf" target="_blank">final paper</a> award at Stanford for CS 238 (<a href="https://web.stanford.edu/class/aa228/cgi-bin/wp/" target="_blank">Decision Making Under Uncertainty)</a>.</li>
                <li>Part of the teaching team that developed and first offered the <a href="https://web.stanford.edu/class/cs329t/" target="_blank">Trustworthy Machine Learning (CS 329T)</a> course at Stanford.
              </ul>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
          <tr>
            <td>
              <heading>Publications</heading>
              <hr>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <heading>Publications</heading> -->
          <tr>
            <td width="30%"><img src="images/tsalm_fig.png" alt="3DSP" width="250" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/abs/2411.09027" target="_blank"><papertitle>Transformer-based Time-Series Biomarker Discovery
                for COPD Diagnosis</papertitle>
              </a>
            <br>
            <strong> Soham Gadgil </strong>,
            Joshua Galanter,
            Mohammadreza Negahdar
            <br>
              <em>NeurIPS 2024 <a href="https://neurips-time-series-workshop.github.io/" target="_blank">TSALM Workshop</a> </em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://arxiv.org/abs/2411.09027" target="_blank"> [Paper]</a>
            </td>
          </tr>
          <tr>
            <td width="30%"><img src="images/concept_figure_protected_attributes.png" alt="3DSP" width="250" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://www.medrxiv.org/content/10.1101/2024.04.09.24305289v1.full.pdf" target="_blank"><papertitle>Discovering mechanisms underlying medical AI prediction of protected
                attributes</papertitle>
              </a>
            <br>
            <strong> Soham Gadgil <sup>*</sup></strong>,
            Alex J. DeGrave <sup>*</sup>,
            Roxana Daneshjou,
            Su-In Lee
            <br>
              <em>CVPR 2024 <a href="https://dca-in-mi.github.io/" target="_blank">DCAMI Workshop</a> <strong>(Oral, Best Paper Runner-Up Award)</strong> </em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://www.medrxiv.org/content/10.1101/2024.04.09.24305289v1.full.pdf" target="_blank"> [Paper]</a>
              <!-- <p align="justify"> Recent advances in Artificial Intelligence (AI) have
                started disrupting the healthcare industry, especially medical imaging, and AI devices are increasingly being deployed
                into clinical practice. Such classifiers have previously
                demonstrated the ability to discern a range of protected demographic attributes (like race, age, sex) from medical images with unexpectedly high performance, a sensitive task
                which is difficult even for trained physicians. Focusing on
                the task of predicting sex from dermoscopic images of skin
                lesions, we are successfully able to train high-performing
                classifiers achieving a ROC-AUC score of ~0.78. We highlight how incorrect use of these demographic shortcuts can
                have a detrimental effect on the performance of a clinically
                relevant downstream task like disease diagnosis under a domain shift. Further, we employ various explainable AI (XAI)
                techniques to identify specific signals which can be leveraged to predict sex. Finally, we introduce a technique to
                quantify how much a signal contributes to the classification
                performance. Using this technique and the signals identified, we are able to explain ~44% of the total performance.
                This analysis not only underscores the importance of cautious AI application in healthcare but also opens avenues
                for improving the transparency and reliability of AI-driven
                diagnostic tools.</p>  -->
            </td>
          </tr>
          <tr>
            <td width="30%"><img src="images/CMI_Concept.jpg" alt="3DSP" width="250" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://arxiv.org/pdf/2306.03301.pdf" target="_blank"><papertitle>Estimating Conditional Mutual Information for Dynamic Feature Selection</papertitle>
              </a>
            <br>
            <strong> Soham Gadgil <sup>*</sup></strong>,
            Ian Covert <sup>*</sup>,
            Su-In Lee
            <br>
              <em>ICLR 2024</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://arxiv.org/pdf/2306.03301.pdf" target="_blank"> [Paper]</a>
              <a href="https://github.com/suinleelab/DIME" target="_blank"> [Code]</a>
              <!-- <p align="justify"> Dynamic feature selection, where we sequentially query features to make accurate
                predictions with a minimal budget, is a promising paradigm to reduce feature
                acquisition costs and provide transparency into the prediction process. The problem is challenging, however, as it requires both making predictions with arbitrary
                feature sets and learning a policy to identify the most valuable selections. Here,
                we take an information-theoretic perspective and prioritize features based on their
                mutual information with the response variable. The main challenge is learning
                this selection policy, and we design a straightforward new modeling approach
                that estimates the mutual information in a discriminative rather than generative
                fashion. Building on our learning approach, we introduce several further improvements: allowing variable feature budgets across samples, enabling non-uniform
                costs between features, incorporating prior information, and exploring modern
                architectures to handle partial input information. We find that our method provides
                consistent gains over recent state-of-the-art methods across a variety of datasets.</p>  -->
            </td>
          </tr>
          <tr>
            <td width="30%"><img src="images/dpfm_fig.png" alt="3DSP" width="250" height="150" style="border-style: none">
              <br>
            <td valign="top" width="70%">
              <a href="https://arxiv.org/pdf/2404.13043" target="_blank"><papertitle>Data Alignment for Zero-Shot Concept Generation in Dermatology AI</papertitle>
              </a>
            <br>
            <strong> Soham Gadgil</strong>,
            Mahtab Bigverdi
            <br>
              <em>ICLR 2024 <a href="https://sites.google.com/view/dpfm-iclr24/home" target="_blank">DPFM workshop</a></em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://arxiv.org/pdf/2404.13043" target="_blank"> [Paper]</a>
              <!-- <p align="justify"> Dynamic feature selection, where we sequentially query features to make accurate
                predictions with a minimal budget, is a promising paradigm to reduce feature
                acquisition costs and provide transparency into the prediction process. The problem is challenging, however, as it requires both making predictions with arbitrary
                feature sets and learning a policy to identify the most valuable selections. Here,
                we take an information-theoretic perspective and prioritize features based on their
                mutual information with the response variable. The main challenge is learning
                this selection policy, and we design a straightforward new modeling approach
                that estimates the mutual information in a discriminative rather than generative
                fashion. Building on our learning approach, we introduce several further improvements: allowing variable feature budgets across samples, enabling non-uniform
                costs between features, incorporating prior information, and exploring modern
                architectures to handle partial input information. We find that our method provides
                consistent gains over recent state-of-the-art methods across a variety of datasets.</p>  -->
            </td>
          </tr>
          <tr>
            <td width="30%"><img src="images/MONET_Concept.png" alt="3DSP" width="250" height="150" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://www.medrxiv.org/content/medrxiv/early/2023/06/12/2023.06.07.23291119.full.pdf" target="_blank"><papertitle>Fostering transparent medical image AI via an image-text foundation model grounded in medical literature</papertitle>
              </a>
            <br>
            Chanwoo Kim,
            <strong>Soham Gadgil</strong>,
            Alex J. DeGrave,
            Zhuo Ran Cai,
            Roxana Daneshjou,
            Su-In Lee
            <br>
              <em>Nature Medicine 2024</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://www.medrxiv.org/content/medrxiv/early/2023/06/12/2023.06.07.23291119.full.pdf" target="_blank"> [Paper]</a>
              <a href="https://github.com/suinleelab/MONET" target="_blank"> [Code]</a>
              <a href="https://www.msn.com/en-gb/health/other/monet-new-ai-tool-enhances-medical-imaging-with-deep-learning-and-text-analysis/ar-AA1niGbn" target="_blank"> [Press]</a>
              <a href="https://news.cs.washington.edu/2024/09/11/monet-helps-paint-a-clearer-picture-of-medical-ai-systems/" target="_blank"> [Allen School News]</a>

              <!-- <p align="justify"> Building trustworthy and transparent image-based medical AI systems requires the ability to interrogate data and
                models at all stages of the development pipeline: from training models to post-deployment monitoring. Ideally,
                the data and associated AI systems could be described using terms already familiar to physicians, but this requires
                medical datasets densely annotated with semantically meaningful concepts. Here, we present a foundation model
                approach, named MONET (Medical cONcept rETriever), which learns how to connect medical images with text and
                generates dense concept annotations to enable tasks in AI transparency from model auditing to model interpretation.
                Dermatology provides a demanding use case for the versatility of MONET, due to the heterogeneity in diseases, skin
                tones, and imaging modalities. We trained MONET on the basis of 105,550 dermatological images paired with natural
                language descriptions from a large collection of medical literature. MONET can accurately annotate concepts across
                dermatology images as verified by board-certified dermatologists, outperforming supervised models built on previously
                concept-annotated dermatology datasets. We demonstrate how MONET enables AI transparency across the entire AI
                development pipeline from dataset auditing to model auditing to building inherently interpretable models.</p>  -->

            </td>
          </tr>
          
          <tr>
            <td width="30%"><img src="images/chexseg_concept.png" alt="3DSP" width="250" height="120" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://proceedings.mlr.press/v143/gadgil21a/gadgil21a.pdf" target="_blank"><papertitle>CheXseg: Combining Expert Annotations with DNN-generated Saliency Maps for X-ray Segmentation</papertitle>
              </a>
            <br>
            <strong>Soham Gadgil</strong> <sup>*</sup>,
            Mark Endo <sup>*</sup>,
            Emily Wen <sup>*</sup>,
            Andrew Y. Ng,
            Pranav Rajpurkar
            <br>
              <em>MIDL 2021</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://proceedings.mlr.press/v143/gadgil21a/gadgil21a.pdf" target="_blank">[Paper]</a>
              <a href="https://github.com/stanfordmlgroup/CheXseg" target="_blank">[Code]</a>

              <!-- <p align="justify"> Medical image segmentation models are typically supervised by expert annotations at the
                pixel-level, which can be expensive to acquire. In this work, we propose a method that
                combines the high quality of pixel-level expert annotations with the scale of coarse DNNgenerated saliency maps for training multi-label semantic segmentation models. We demonstrate the application of our semi-supervised method, which we call CheXseg, on multilabel chest X-ray interpretation. We find that CheXseg improves upon the performance
                (mIoU) of fully-supervised methods that use only pixel-level expert annotations by 9.7%
                and weakly-supervised methods that use only DNN-generated saliency maps by 73.1%. Our
                best method is able to match radiologist agreement on three out of ten pathologies and
                reduces the overall performance gap by 57.2% as compared to weakly-supervised methods.</p>  -->

            </td>
          </tr>
          
          <tr>
            <td width="30%"><img src="images/stgcn_concept.png" alt="3DSP" width="250" height="120" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7700758/pdf/nihms-1645484.pdf" target="_blank"><papertitle>Spatio-Temporal Graph Convolution for Resting-State fMRI Analysis</papertitle>
              </a>
            <br>
            <strong>Soham Gadgil</strong> <sup>*</sup>,
            Qingyu Zhao <sup>*</sup>,
            Adolf Pfefferbaum <sup>*</sup>,
            Edith V. Sullivan,
            Ehsan Adeli,
            Kilian M. Pohl
            <br>
              <em>MICCAI 2020</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7700758/pdf/nihms-1645484.pdf" target="_blank">[Paper]</a>
              <a href="https://github.com/sgadgil6/cnslab_fmri" target="_blank">[Code]</a>

              <!-- <p align="justify"> The Blood-Oxygen-Level-Dependent (BOLD) signal of resting-state fMRI (rs-fMRI) records the
                temporal dynamics of intrinsic functional networks in the brain. However, existing deep learning
                methods applied to rs-fMRI either neglect the functional dependency between different brain
                regions in a network or discard the information in the temporal dynamics of brain activity. To
                overcome those shortcomings, we propose to formulate functional connectivity networks within
                the context of spatio-temporal graphs. We train a spatio-temporal graph convolutional network
                (ST-GCN) on short sub-sequences of the BOLD time series to model the non-stationary nature of
                functional connectivity. Simultaneously, the model learns the importance of graph edges within
                ST-GCN to gain insight into the functional connectivities contributing to the prediction. In
                analyzing the rs-fMRI of the Human Connectome Project (HCP, N = 1,091) and the National
                Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA, N = 773), ST-GCN is
                significantly more accurate than common approaches in predicting gender and age based on
                BOLD signals. Furthermore, the brain regions and functional connections significantly
                contributing to the predictions of our model are important markers according to the neuroscience
                literature.</p>  -->

            </td>
          </tr>

          <tr>
            <td width="30%"><img src="images/dqn_fig.png" alt="3DSP" width="250" height="120" style="border-style: none">
            <td valign="top" width="70%">
              <a href="https://ieeexplore.ieee.org/abstract/document/9368267" target="_blank"><papertitle>Solving The Lunar Lander Problem under Uncertainty using Reinforcement Learning
              </papertitle>
              </a>
            <br>
            <strong>Soham Gadgil</strong>,
              Yunfeng Xin,
              Chengzhe Xu
            <br>
              <em>IEEE SouthEastCon 2020</em> 
              <!-- <font color="red">(Poster)</font> -->
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9368267" target="_blank">[Paper]</a>
              <a href="https://github.com/sgadgil6/lunar_lander_project" target="_blank">[Code]</a>
              <!-- <p align="justify"> Reinforcement Learning (RL) is an area of machine learning concerned with enabling an agent to navigate an environment with uncertainty in order to maximize some notion of cumulative long-term reward. In this paper, we implement and analyze two different RL techniques, Sarsa and Deep QLearning, on OpenAI Gym's LunarLander-v2 environment. We then introduce additional uncertainty to the original problem to test the robustness of the mentioned techniques. With our best models, we are able to achieve average rewards of 170+ with the Sarsa agent and 200+ with the Deep Q-Learning agent on the original problem. We also show that these techniques are able to overcome the additional uncertainities and achieve positive average rewards of 100+ with both agents. We then perform a comparative analysis of the two techniques to conclude which agent peforms better.</p>  -->

            </td>
          </tr>


        </tbody></table>


        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td>
              <heading>Other Experiences</heading>

        <hr>
              <ul>
                <li>Reviewer: NeurIPS 2024, MIDL 2022, ICLR 2024 DPFM Workshop</li>
                <li>President of Georgia Tech IEEE, leading the largest IEEE student branch in the US with over 800 members.</li>
                <li>International Liasion for the Student Alumni Association at Georgia Tech.</li>
                <li>Peer leader in freshmen and senior student dormitories.</li>
                
              </ul>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
          <tr>
            <td>
              <heading>Teaching Assistantships</heading>
              
            </td>
          </tr>
        </tbody></table>
        <hr>
        <table width="90%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
          <tr>
            <td>
            <h5>University of Washington</h5>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <heading>Publications</heading> -->
          <tr>
            <td width="30%"><img src="images/cse473.jpeg" alt="cs_107" width="200" height="150" style="border-style: 2px">
            <td valign="top" width="70%">
              
                      <papertitle>Introduction to AI (<a href="https://courses.cs.washington.edu/courses/cse473/22au/" target="_blank">CSE 473</a>)</papertitle>
            
            <br>
              <!-- <font color="red">(Poster)</font> -->

              <p align="justify">The course covers principal ideas and developments in artificial intelligence: Problem solving and search, game playing, knowledge representation and reasoning, uncertainty, machine learning, natural language processing. I held weekly office hours and assisted in preparing/grading the homework assignments.</p> 
            </td>
          </tr>

        </tbody></table>


        <table width="90%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
          <tr>
            <td>
            <h5>Stanford</h5>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <heading>Publications</heading> -->
          <tr>
            <td width="30%"><img src="images/cs_107_img.png" alt="cs_107" width="150" height="150" style="border-style: 2px">
            <td valign="top" width="70%">
              
                      <papertitle>Computer Organization and Systems (<a href="http://web.stanford.edu/class/cs107/" target="_blank">CS 107</a>)</papertitle>
            
            <br>
              <!-- <font color="red">(Poster)</font> -->

              <p align="justify">TA for CS 107, one of the largest introductory undergraduate courses at Stanford with over 150 students. I led two hour-long lab sessions each week along with office hours and assisted the professor in grading homework and desiging exams. Topics included the C programming language, data representation, machine-level code, computer arithmetic, elements of code compilation, optimization of memory and runtime performance, and memory organization and management.</p> 
            </td>
          </tr>

          <tr></tr>
          <tr>
            <td width="30%"><img src="images/trust_ml.jpg" alt="iswc2018" width="200" height="170" style="border-style: none">
            <td valign="top" width="70%">
                      <papertitle>Trustworthy Machine Learning (<a href="https://web.stanford.edu/class/cs329t/" target="_blank">CS 329T</a>)</papertitle>
              <p align="justify"> TA for the first course offering of CS 329T. I co-developed and led the lab sections with ~25 students. I also helped the instructors design some of the lecture slides, homework assignments, and the final project.</p> 
            </td>
          </tr>

        </tbody></table>
        <table width="90%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
          <tr>
            <td>
              <h5>Georgia Tech</h5>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <heading>Publications</heading> -->
          <tr>
            <td width="30%"><img src="images/linear_algebra.png" alt="linear_alg" width="220" height="150" style="border-style: none">
            <td valign="top" width="70%">
              
                      <papertitle>Linear Algebra (<a href="https://math.gatech.edu/courses/math/1554" target="_blank">MATH 1554</a>)</papertitle>
            
            <br>
              <!-- <font color="red">(Poster)</font> -->

              <p align="justify">As a TA for linear algebra, I led two 50 minute recitation sessions with 25 students each week. Concepts ranged from eigenvalues, eigenvectors, applications to linear systems, least squares, diagonalization, quadratic forms. </p> 
            </td>
          </tr>

          <tr></tr>
          <tr>
            <td width="30%"><img src="images/comp_arch.png" alt="comp_arch" width="200" height="100" style="border-style: none">
            <td valign="top" width="70%">
                      <papertitle>Computer Architecture (<a href="https://tusharkrishna.ece.gatech.edu/teaching/uca_f18/" target="_blank">CS 3056</a>)</papertitle>
              <p align="justify"> Guided over 60 students with homeworks and projects in computer architecture. Held weekly office hours, exam review sessions, and collaborated with the instructor for grading and project ideation. Topics included the basic organizational principles of the major components of a processor - the core, memory hierarchy, and the I/O subsystem.</p> 
            </td>
          </tr>

        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
                  <hr>
                  <p align="center">
                  <font>(Design and CSS courtesy: <a href="https://jonbarron.info/" target="_blank">Jon Barron</a> and <a href="https://abhoi.github.io/" target="_blank">Amlaan Bhoi</a>)</font>
                  </p>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

  
</body>

</html>
